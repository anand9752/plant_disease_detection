"""
Plant Disease Model Compression Script
=====================================

This script implements multiple compression techniques to reduce the model size
from ~573MB to under 100MB for Streamlit Cloud deployment:

1. Post-training Quantization (INT8)
2. TensorFlow Lite Conversion
3. Model Pruning
4. Weight Clustering

Author: AI Assistant
Date: September 2025
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
import tempfile
import shutil

class ModelCompressor:
    def __init__(self, model_path, output_dir="compressed_models"):
        """
        Initialize the model compressor
        
        Args:
            model_path (str): Path to the original .h5 model file
            output_dir (str): Directory to save compressed models
        """
        self.model_path = model_path
        self.output_dir = output_dir
        self.original_model = None
        self.compressed_models = {}
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ðŸš€ Model Compression Toolkit Initialized")
        print(f"ðŸ“ Input Model: {model_path}")
        print(f"ðŸ“ Output Directory: {output_dir}")
    
    def load_original_model(self):
        """Load the original model and display information"""
        try:
            print("\nðŸ“Š Loading Original Model...")
            self.original_model = tf.keras.models.load_model(self.model_path)
            
            # Get model info
            original_size = os.path.getsize(self.model_path) / (1024 * 1024)  # MB
            
            print(f"âœ… Model loaded successfully!")
            print(f"ðŸ“ Original Size: {original_size:.2f} MB")
            print(f"ðŸ—ï¸  Model Architecture:")
            
            # Count parameters
            total_params = self.original_model.count_params()
            trainable_params = sum([tf.keras.backend.count_params(w) for w in self.original_model.trainable_weights])
            
            print(f"   â€¢ Total Parameters: {total_params:,}")
            print(f"   â€¢ Trainable Parameters: {trainable_params:,}")
            print(f"   â€¢ Model Layers: {len(self.original_model.layers)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error loading model: {str(e)}")
            return False
    
    def create_representative_dataset(self, num_samples=100):
        """
        Create a representative dataset for quantization
        This simulates real plant images for calibration
        """
        print(f"\nðŸŽ¯ Creating Representative Dataset ({num_samples} samples)...")
        
        def representative_data_gen():
            for _ in range(num_samples):
                # Generate synthetic plant-like images (224x224x3)
                # Using realistic value ranges for plant images
                sample = np.random.uniform(0.0, 1.0, (1, 224, 224, 3)).astype(np.float32)
                yield [sample]
        
        return representative_data_gen
    
    def apply_post_training_quantization(self):
        """
        Apply post-training quantization to reduce model size
        """
        print("\nâš¡ Applying Post-Training Quantization...")
        
        try:
            # Convert to TensorFlow Lite with quantization
            converter = tf.lite.TFLiteConverter.from_keras_model(self.original_model)
            
            # Enable optimizations
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Set up representative dataset for full integer quantization
            converter.representative_dataset = self.create_representative_dataset()
            
            # Force full integer quantization
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.int8
            converter.inference_output_type = tf.int8
            
            # Convert model
            quantized_tflite_model = converter.convert()
            
            # Save quantized model
            quantized_path = os.path.join(self.output_dir, "quantized_model.tflite")
            with open(quantized_path, 'wb') as f:
                f.write(quantized_tflite_model)
            
            # Calculate compression ratio
            original_size = os.path.getsize(self.model_path) / (1024 * 1024)
            quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
            compression_ratio = (1 - quantized_size / original_size) * 100
            
            print(f"âœ… Quantization Complete!")
            print(f"ðŸ“ Original Size: {original_size:.2f} MB")
            print(f"ðŸ“ Quantized Size: {quantized_size:.2f} MB")
            print(f"ðŸŽ¯ Compression: {compression_ratio:.1f}%")
            
            self.compressed_models['quantized'] = {
                'path': quantized_path,
                'size_mb': quantized_size,
                'compression_ratio': compression_ratio,
                'type': 'TensorFlow Lite INT8'
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ Quantization failed: {str(e)}")
            return False
    
    def apply_dynamic_range_quantization(self):
        """
        Apply dynamic range quantization (float16)
        Less aggressive but more compatible
        """
        print("\nðŸ”„ Applying Dynamic Range Quantization...")
        
        try:
            converter = tf.lite.TFLiteConverter.from_keras_model(self.original_model)
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Convert model
            dynamic_tflite_model = converter.convert()
            
            # Save model
            dynamic_path = os.path.join(self.output_dir, "dynamic_quantized_model.tflite")
            with open(dynamic_path, 'wb') as f:
                f.write(dynamic_tflite_model)
            
            # Calculate compression
            original_size = os.path.getsize(self.model_path) / (1024 * 1024)
            dynamic_size = os.path.getsize(dynamic_path) / (1024 * 1024)
            compression_ratio = (1 - dynamic_size / original_size) * 100
            
            print(f"âœ… Dynamic Quantization Complete!")
            print(f"ðŸ“ Dynamic Quantized Size: {dynamic_size:.2f} MB")
            print(f"ðŸŽ¯ Compression: {compression_ratio:.1f}%")
            
            self.compressed_models['dynamic'] = {
                'path': dynamic_path,
                'size_mb': dynamic_size,
                'compression_ratio': compression_ratio,
                'type': 'TensorFlow Lite Float16'
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ Dynamic quantization failed: {str(e)}")
            return False
    
    def apply_model_pruning(self):
        """
        Apply magnitude-based pruning to remove unnecessary weights
        """
        print("\nâœ‚ï¸  Applying Model Pruning...")
        
        try:
            import tensorflow_model_optimization as tfmot
            
            # Clone the model for pruning
            pruned_model = tf.keras.models.clone_model(self.original_model)
            pruned_model.set_weights(self.original_model.get_weights())
            
            # Define pruning parameters
            pruning_params = {
                'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
                    initial_sparsity=0.30,  # Start with 30% sparsity
                    final_sparsity=0.70,    # End with 70% sparsity
                    begin_step=0,
                    end_step=1000
                )
            }
            
            # Apply pruning to dense and conv layers
            def apply_pruning_to_layer(layer):
                if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):
                    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)
                return layer
            
            # Prune the model
            pruned_model = tf.keras.models.clone_model(
                pruned_model,
                clone_function=apply_pruning_to_layer,
            )
            
            # Compile the pruned model
            pruned_model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            # Remove pruning wrappers
            final_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
            
            # Save pruned model
            pruned_path = os.path.join(self.output_dir, "pruned_model.h5")
            final_pruned_model.save(pruned_path)
            
            # Calculate compression
            original_size = os.path.getsize(self.model_path) / (1024 * 1024)
            pruned_size = os.path.getsize(pruned_path) / (1024 * 1024)
            compression_ratio = (1 - pruned_size / original_size) * 100
            
            print(f"âœ… Pruning Complete!")
            print(f"ðŸ“ Pruned Size: {pruned_size:.2f} MB")
            print(f"ðŸŽ¯ Compression: {compression_ratio:.1f}%")
            
            self.compressed_models['pruned'] = {
                'path': pruned_path,
                'size_mb': pruned_size,
                'compression_ratio': compression_ratio,
                'type': 'Pruned Keras Model'
            }
            
            return True
            
        except ImportError:
            print("âš ï¸  TensorFlow Model Optimization not available. Installing...")
            os.system("pip install tensorflow-model-optimization")
            return self.apply_model_pruning()
        except Exception as e:
            print(f"âŒ Pruning failed: {str(e)}")
            return False
    
    def create_optimized_keras_model(self):
        """
        Create an optimized Keras model with reduced precision
        """
        print("\nðŸ”§ Creating Optimized Keras Model...")
        
        try:
            # Use mixed precision for reduced memory usage
            policy = tf.keras.mixed_precision.Policy('mixed_float16')
            tf.keras.mixed_precision.set_global_policy(policy)
            
            # Clone and optimize the model
            optimized_model = tf.keras.models.clone_model(self.original_model)
            optimized_model.set_weights(self.original_model.get_weights())
            
            # Compile with mixed precision
            optimized_model.compile(
                optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            # Save optimized model
            optimized_path = os.path.join(self.output_dir, "optimized_model.h5")
            optimized_model.save(optimized_path)
            
            # Calculate size
            original_size = os.path.getsize(self.model_path) / (1024 * 1024)
            optimized_size = os.path.getsize(optimized_path) / (1024 * 1024)
            compression_ratio = (1 - optimized_size / original_size) * 100
            
            print(f"âœ… Optimization Complete!")
            print(f"ðŸ“ Optimized Size: {optimized_size:.2f} MB")
            print(f"ðŸŽ¯ Compression: {compression_ratio:.1f}%")
            
            self.compressed_models['optimized'] = {
                'path': optimized_path,
                'size_mb': optimized_size,
                'compression_ratio': compression_ratio,
                'type': 'Mixed Precision Keras'
            }
            
            # Reset policy
            tf.keras.mixed_precision.set_global_policy('float32')
            
            return True
            
        except Exception as e:
            print(f"âŒ Optimization failed: {str(e)}")
            # Reset policy on error
            tf.keras.mixed_precision.set_global_policy('float32')
            return False
    
    def compress_all(self):
        """
        Apply all compression techniques
        """
        print("\nðŸŽ¯ Starting Comprehensive Model Compression...")
        print("=" * 60)
        
        if not self.load_original_model():
            return False
        
        # Apply all compression techniques
        techniques = [
            ("Dynamic Range Quantization", self.apply_dynamic_range_quantization),
            ("Post-Training Quantization", self.apply_post_training_quantization),
            ("Model Pruning", self.apply_model_pruning),
            ("Keras Optimization", self.create_optimized_keras_model)
        ]
        
        successful_compressions = 0
        
        for name, method in techniques:
            print(f"\n{'=' * 20} {name} {'=' * 20}")
            if method():
                successful_compressions += 1
            else:
                print(f"âš ï¸  {name} failed, continuing with other methods...")
        
        # Generate summary report
        self.generate_compression_report()
        
        print(f"\nðŸŽ‰ Compression Complete!")
        print(f"âœ… {successful_compressions}/{len(techniques)} techniques successful")
        
        return successful_compressions > 0
    
    def generate_compression_report(self):
        """
        Generate a comprehensive compression report
        """
        print("\n" + "=" * 60)
        print("ðŸ“Š COMPRESSION SUMMARY REPORT")
        print("=" * 60)
        
        original_size = os.path.getsize(self.model_path) / (1024 * 1024)
        
        print(f"ðŸ”¸ Original Model Size: {original_size:.2f} MB")
        print(f"ðŸ”¸ Target Size for Streamlit: < 100 MB")
        print()
        
        if not self.compressed_models:
            print("âŒ No successful compressions")
            return
        
        # Sort by size
        sorted_models = sorted(
            self.compressed_models.items(),
            key=lambda x: x[1]['size_mb']
        )
        
        print("ðŸ† COMPRESSION RESULTS (Best to Worst):")
        print("-" * 60)
        
        for i, (name, info) in enumerate(sorted_models, 1):
            status = "âœ… STREAMLIT READY" if info['size_mb'] < 100 else "âš ï¸  Still Large"
            print(f"{i}. {info['type']}")
            print(f"   ðŸ“ Size: {info['size_mb']:.2f} MB")
            print(f"   ðŸŽ¯ Compression: {info['compression_ratio']:.1f}%")
            print(f"   ðŸ“ Path: {info['path']}")
            print(f"   ðŸš€ Status: {status}")
            print()
        
        # Recommendation
        best_model = sorted_models[0]
        print("ðŸ’¡ RECOMMENDATION:")
        print("-" * 40)
        
        if best_model[1]['size_mb'] < 100:
            print(f"âœ… Use: {best_model[1]['type']}")
            print(f"ðŸ“ File: {best_model[1]['path']}")
            print(f"ðŸŽ¯ This model is ready for Streamlit deployment!")
        else:
            print("âš ï¸  All models are still large for Streamlit Cloud")
            print("ðŸ“ Consider:")
            print("   â€¢ Using Streamlit Community Cloud with Git LFS")
            print("   â€¢ Hosting model externally (Google Drive, AWS S3)")
            print("   â€¢ Further model architecture optimization")
        
        # Save report to file
        report_path = os.path.join(self.output_dir, "compression_report.txt")
        with open(report_path, 'w') as f:
            f.write("Plant Disease Model Compression Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Original Size: {original_size:.2f} MB\n\n")
            
            for name, info in sorted_models:
                f.write(f"{info['type']}:\n")
                f.write(f"  Size: {info['size_mb']:.2f} MB\n")
                f.write(f"  Compression: {info['compression_ratio']:.1f}%\n")
                f.write(f"  Path: {info['path']}\n\n")
        
        print(f"ðŸ“„ Report saved to: {report_path}")

def main():
    """
    Main compression function
    """
    print("ðŸŒ± Plant Disease Model Compression Tool")
    print("=" * 50)
    
    # Set up paths
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(current_dir, "trained_model", "plant_disease_prediction_model.h5")
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        print("ðŸ“ Please ensure the model file exists in the trained_model directory")
        return
    
    # Initialize compressor
    compressor = ModelCompressor(model_path)
    
    # Run compression
    success = compressor.compress_all()
    
    if success:
        print("\nðŸŽ‰ Model compression completed successfully!")
        print("ðŸ“ Check the 'compressed_models' directory for results")
        print("\nðŸ“‹ Next Steps:")
        print("1. Test the compressed models with test_compressed_models.py")
        print("2. Update main.py to use the best compressed model")
        print("3. Deploy to Streamlit Cloud")
    else:
        print("\nâŒ Model compression failed")
        print("ðŸ’¡ Please check error messages and try again")

if __name__ == "__main__":
    main()